{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1918672c-efff-4a7e-b2de-20dd0c69d3cc",
   "metadata": {},
   "source": [
    "Author: William Murphy\n",
    "Date: 12/15/2023\n",
    "Description: This code is ment to answer analysis task 1 below.\n",
    "Version: 1.01\n",
    "\n",
    "Analysis task 1:  \n",
    "\n",
    "If a tag is present in an image, how many other people added that tag anywhere in the image divided by the total number of crowd volunteers that labeled the image. \n",
    "\n",
    "Topics of interest:\n",
    "Some people have said that an image was phishy, but did not drawn any bounding boxes on their answer.\n",
    "\n",
    "\n",
    "Version: 1.00\n",
    "Desc:   Version 1.01 creates a table of ratios of all the cue types.\n",
    "        Removes duplicates based on 3 factors, the individual user, the image, and the cue type.\n",
    "        Duplicates are defined as \"Bounding boxes that were drawn by a single individual on a single image that have the same cue type.\"\n",
    "        The ratios are (the number of people that gave a certain cue at least once) / (the total number of people that labeled an image)\n",
    "        \n",
    "Steps:\n",
    "1. Get Gold Standard images\n",
    "2. Get subject_ids and Bounding Box values\n",
    "3. Sorting Dataframe separting the different cues given in json format\n",
    "4. Removing duplicate cues given by a single user\n",
    "5. Getting the total number of cues of each type sorted by the image\n",
    "\n",
    "6. Merging gold_std and cues together\n",
    "7. Getting the total number of people that access an image\n",
    "8. Sort data into percentages\n",
    "\n",
    "Version 1.01\n",
    "Desc:   Version 1.01 creates another table of ratios of all cue types.\n",
    "        Removed duplicates is still defined the same as in version 1.00\n",
    "        The new ratios for the cues will be defined as \n",
    "        (the number of people that gave a certain cue at least once) / (the total number of cues given on a certain image)\n",
    "\n",
    "Steps:\n",
    "1. Getting the total number of none duplicate cues for every image\n",
    "2. Mergeing number of cue types per image with the total number of cues per image\n",
    "3. Making the ratio table\n",
    "\n",
    "Version 1.02\n",
    "Desc:   Version 1.02 creates another table of ratios of all cue types.\n",
    "        Duplicates will now be defined as \"Bounding boxes that are intersecting and of the same cue type\"\n",
    "        The assumation will be made that the above did happen, the Bounding boxes could have been represented by just one box.\n",
    "        The new ratios will for the cues will be defined as\n",
    "        (the total number of cues given of a certain type) / (the total number of cues given on a certain image)\n",
    "\n",
    "Analysis task 2:\n",
    "\n",
    "Rank all tags by their Relative Average Ratio across the entire data set. \n",
    "An intuition of meaning here might be the relative stability or ease of detection of the cue.\n",
    "\n",
    "Question:\n",
    "        What does ranked mean?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c66fb01",
   "metadata": {},
   "source": [
    "2027 Total Images in CyberTrust\n",
    "1698 Total Images have been labeled at least once\n",
    "329 Total Images are unmarked\n",
    "1779 entries where people didn't include a cue type out of 12743"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e45138",
   "metadata": {},
   "source": [
    "Connecting Notebook to Datebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3745eed8-c2c1-4be1-8281-b42c7685e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to download any necessary modules\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install psycopg2\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install sqlalchemy\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install matplotlib\n",
    "#!{sys.executable} -m pip install scipy\n",
    "#!{sys.executable} -m pip install scikit_posthocs\n",
    "#!{sys.executable} -m pip install termcolor\n",
    "#!{sys.executable} -m pip install warnings\n",
    "#!{sys.executable} -m pip install plotnine\n",
    "#!{sys.executable} -m pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f222303-f65c-4e22-84ae-ecce5406fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 #for database connection\n",
    "import pandas as pd \n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import json\n",
    "\n",
    "# Kruskal-Wallis analysis of variance\n",
    "import scipy.stats as ss # For Kruskal-Wallis test\n",
    "import scikit_posthocs as sp #For post hoc tests. \n",
    "from termcolor import colored # for coloring the print text\n",
    "import warnings # to ignore plot warnings\n",
    "\n",
    "# For ggplot\n",
    "from plotnine import *\n",
    "\n",
    "# For bolding the printed text\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d335abc7-7b96-4eea-8438-80d0b039f76d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Datebase code was here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a8144b-c5e6-41cb-a700-c702cbb791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Connection\n",
    "try:\n",
    "    connection = psycopg2.connect( host=hostname, user=username, password=password, dbname=database, port=port )\n",
    "    \n",
    "    \n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896e1b7e-d55d-4074-a4e1-05aeb73b10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 10)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a0ece",
   "metadata": {},
   "source": [
    "Beginning of Version 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5ffd2-234e-44b8-8695-f0ee05bf7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Getting gold standard images from phase 1\n",
    "gold_std = pd.read_sql_query(\"select subject_id_ph1 as subject_id, filename, malicious, gold_std from cybertrust_zooniverse_datamatch where gold_std is true\", connection)\n",
    "gold_std\n",
    "\n",
    "gold_std = gold_std.dropna(subset= ['subject_id']).reset_index()\n",
    "gold_std = gold_std.drop('index', axis=1)\n",
    "gold_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ed277-1ccf-4928-bd6c-d41fdaefd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Getting subject_id and raw json strings that hold the user's bounding boxes\n",
    "# The code technically gets all the entries where a user select that an image looked fishy.\n",
    "# It does this by getting the subject id and the bounding boxes drawn by a user for that subject id.\n",
    "\n",
    "# If you see an entry in the table below that has \"[]\" in the user_answer column,\n",
    "# that would be an instance where the user answered that the image was phishy, but did not draw any bounding boxes\n",
    "\n",
    "sql = \"\"\"select c.subject_ids as subject_id, a.annotations->>'value' as user_answers, b.annotations->> 'value' as phishing_class \n",
    "       from zooniverse_phish_classifications as c, \n",
    "    jsonb_array_elements(c.annotations) as a(annotations),\n",
    "    jsonb_array_elements(c.annotations) as b(annotations)\n",
    "    where a.annotations->>'task' = 'T0' and \n",
    "        b.annotations->>'task' = 'T3'\n",
    "\"\"\"\n",
    "\n",
    "zooniverseclassification = pd.read_sql_query(sql, connection)\n",
    "zooniverseclassification\n",
    "\n",
    "#zooniverseclassification = zooniverseclassification[zooniverseclassification['user_answers'] == '[]']\n",
    "#zooniverseclassification[zooniverseclassification['subject_id'] == 43857067]\n",
    "zooniverseclassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Zoon_malicious= zooniverseclassification.groupby('subject_id').apply(lambda x: pd.Series(\n",
    "    dict( Total_People_who_put_malicious = (x.phishing_class == \"Something's Phishy\").sum()\n",
    "    ))).reset_index('subject_id')\n",
    "\n",
    "Zoon_malicious\n",
    "\n",
    "change_type ={\"subject_id\": \"int64\"}\n",
    "Zoon_malicious = Zoon_malicious.astype(change_type)\n",
    "data_types = Zoon_malicious.dtypes\n",
    "data_types\n",
    "\n",
    "Zoon_malicious = pd.merge(gold_std, Zoon_malicious, on=\"subject_id\")\n",
    "Zoon_malicious\n",
    "\n",
    "\n",
    "\n",
    "#Zoon_malicious.to_csv(\"Zoon_malicious.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_cue_type= zooniverseclassification[zooniverseclassification['user_answers'] == '[]']\n",
    "Count_No_Cue_Type = No_cue_type.groupby('subject_id').apply(lambda x: pd.Series(\n",
    "    dict(User_who_did_not_put_cue = (x.user_answers == '[]').sum()\n",
    "    ))).reset_index('subject_id')\n",
    "\n",
    "Count_No_Cue_Type= pd.DataFrame(Count_No_Cue_Type)\n",
    "Count_No_Cue_Type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type ={\"subject_id\": \"int64\"}\n",
    "Count_No_Cue_Type = Count_No_Cue_Type.astype(change_type)\n",
    "data_types = Count_No_Cue_Type.dtypes\n",
    "data_types\n",
    "\n",
    "No_Cue_Gold_Standard = pd.merge(gold_std, Count_No_Cue_Type, on=\"subject_id\")\n",
    "No_Cue_Gold_Standard\n",
    "\n",
    "#No_Cue_Gold_Standard.to_csv(\"No_Cue_Gold_Standard.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec1967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 \n",
    "# Sorting Dataframe separting the different cues given in json format\n",
    "# This dataframe stores all the information related to the bounded boxes.\n",
    "# User_labeled_image is one user's session of labeling one certain image to search for unique cues given by any user\n",
    "usercues = pd.DataFrame(columns=[\"user_labeled_image\", \"subject_id\", \"cue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac04efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could takes all user bounding boxes and sorts them by \n",
    "# user_labeled_image(unique user interaction with unique image), \n",
    "# subject_id(image), and cues.\n",
    "# This doesn't not filter out duplicate cue from a user on a certain image\n",
    "\n",
    "for user_labeled_image in range(0, len(zooniverseclassification[\"user_answers\"])):\n",
    "    current_string = zooniverseclassification[\"user_answers\"][user_labeled_image]\n",
    "    json_object = json.loads(current_string)\n",
    "    \n",
    "    for item in json_object:\n",
    "        data_input = {\"user_labeled_image\": user_labeled_image, \n",
    "                            \"subject_id\": zooniverseclassification[\"subject_id\"][user_labeled_image], \n",
    "                            \"cue\": item[\"tool_label\"]}   \n",
    "        \n",
    "        usercues.loc[len(usercues.index)] = data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ac598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe made from code above\n",
    "# One error \"user_labeled_image\" for 0 is repeated\n",
    "usercues\n",
    "\n",
    "unique_terms = usercues['cue'].unique()\n",
    "unique_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c7e9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60af9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Filtering out duplicate cue given by a single user\n",
    "usercues = usercues.drop_duplicates()\n",
    "\n",
    "check = usercues[usercues['subject_id'].isin(['49251699', '49251697', '49251696', '49251694', '49251678', '49251693',\n",
    "'49251691',\n",
    "'49251690',\n",
    "'49251687',\n",
    "'49251675',\n",
    "'49251686',\n",
    "'49251683',\n",
    "'49251682',\n",
    "'49251681',\n",
    "'49251704',\n",
    "'49251703',\n",
    "'49251702',\n",
    "'49251701',\n",
    "'49251700',\n",
    "'49251698',\n",
    "'49251695',\n",
    "'49251677',\n",
    "'49251692',\n",
    "'49251676',\n",
    "'49251689',\n",
    "'49251688',\n",
    "'49251685',\n",
    "'49251684',\n",
    "'49251680',\n",
    "'49251679'\n",
    "])]\n",
    "\n",
    "check[check['cue'].isin(['Appeal to Action-Urgency'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2f93c-bea0-4afb-9f4b-ac1ba0662d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5 \n",
    "# Getting the total number of cues of each type sorted by the image\n",
    "# Sorting the cues given by unique users\n",
    "sortedcues = usercues.groupby('subject_id').apply(lambda x: pd.Series(\n",
    "    dict(Invalid_Domain_or_Sender = (x.cue == \"Invalid Domain or Sender\").sum(),\n",
    "         Potent_Mal_Links = (x.cue == \"Potentially Malicious Link\").sum(),\n",
    "         Spelling_or_Grammar = (x.cue == \"Poor Spelling or Grammar\").sum(),\n",
    "         Appeal_to_Greed = (x.cue == \"Appeal to Action-Greed\").sum(),\n",
    "         Appeal_to_Urgency = (x.cue == \"Appeal to Action-Urgency\").sum(),\n",
    "         Appeal_to_Authority = (x.cue == \"Appeal to Action-Authority\").sum(),\n",
    "         Other_Phishy_Findings = (x.cue == \"Other Phishy Findings\").sum()\n",
    "    ))).reset_index('subject_id')\n",
    "\n",
    "sortedcues = pd.DataFrame(sortedcues)\n",
    "sortedcues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e9ea2-9118-4ce8-8151-2b854657526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "# Merging table together\n",
    "#Changing subject_id from object to int64\n",
    "change_type ={\"subject_id\": \"int64\"}\n",
    "sortedcues = sortedcues.astype(change_type)\n",
    "data_types = sortedcues.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c52ca9-ec01-4816-8eb2-6fb27641654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedcues_gold_std = pd.merge(gold_std, sortedcues, on=\"subject_id\")\n",
    "sortedcues_gold_std\n",
    "\n",
    "#sortedcues_gold_std[sortedcues_gold_std['subject_id'] == 43857076]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5869f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048803f-309b-4e72-b990-a6f05d63567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 \n",
    "# Getting the total number of people that accessed any image\n",
    "# Trust or not trust for each task id i.e. image classification on zooniverse\n",
    "sql = \"\"\"select c.subject_ids as subject_id, a.annotations->>'value' as zoo_trust \n",
    "       from zooniverse_phish_classifications as c, \n",
    "    jsonb_array_elements(c.annotations) as a(annotations)\n",
    "    where a.annotations->>'task' = 'T3'\"\"\"\n",
    "\n",
    "classificationsession = pd.read_sql_query(sql, connection)\n",
    "classificationsession['subject_id'] = classificationsession['subject_id'].astype('int64')\n",
    "\n",
    "#classificationsession['zoo_trust_b'] = (classificationsession['zoo_trust'] == \"Something's Phishy\")\n",
    "\n",
    "classificationsession[classificationsession['zoo_trust'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b82ee-2b69-461d-8342-344478a39a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the total number of people who labeled an image\n",
    "z_total_people_df = classificationsession.groupby('subject_id').apply(lambda x: pd.Series(\n",
    "               dict(z_no_trust = (x.zoo_trust == \"Something's Phishy\").sum(),\n",
    "                    z_trust = (x.zoo_trust == 'Nothing Phishy Here').sum(),\n",
    "                    z_no_answer = (x.zoo_trust.isna()).sum(),\n",
    "                   z_total_people = (x.zoo_trust == \"Something's Phishy\").sum()+ (x.zoo_trust == 'Nothing Phishy Here').sum()))).reset_index('subject_id')\n",
    "z__total_people_df = pd.DataFrame(z_total_people_df)\n",
    "\n",
    "\n",
    "\n",
    "#z_total_people_df[z__total_people_df['subject_id'] == 43857076]\n",
    "\n",
    "z__total_people_df\n",
    "#gold_std_1= pd.merge(gold_std, z_total_people_df, on= \"subject_id\")\n",
    "#gold_std_1\n",
    "\n",
    "#gold_std_1.to_csv(\"Z.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_total_people_df\n",
    "phase1= pd.read_sql_query(\"select subject_id_ph1 as subject_id, filename, malicious, gold_std from cybertrust_zooniverse_datamatch\", connection)\n",
    "\n",
    "phase_2= pd.merge(phase1, z_total_people_df, on= \"subject_id\")\n",
    "phase_2\n",
    "\n",
    "#phase_2.to_csv(\"All_Images_with_malicious_or_not.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb0c63-0696-4862-81be-0d066bad9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Databases together\n",
    "sortedcues_total_people = pd.merge(sortedcues_gold_std, z_total_people_df, on= \"subject_id\")\n",
    "sortedcues_total_people\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6349a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sortedcues_total_people.to_csv(\"Distinct_people_cue_gld_std.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159352f8",
   "metadata": {},
   "source": [
    "Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba397424-b1c2-4278-b19b-90a1808dc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "# Sorting data into percentage\n",
    "# The sum method is used at the end of all the calculations to remove formating problems\n",
    "z_cue_percentages_by_people = sortedcues_total_people.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict( filename = x.filename.sum(),\n",
    "          malicious = (x.malicious.sum()  == 1),\n",
    "          gold_std = (x.gold_std.sum() == 1),\n",
    "          Invalid_Domain_Sender_Ratio = format((x.Invalid_Domain_or_Sender / x.z_total_people).sum(),'.2%'),\n",
    "          Potential_Malicious_Links_Ratio = format((x.Potent_Mal_Links / x.z_total_people).sum(), '.2%'),\n",
    "          Poor_Spelling_or_Grammar_Ratio = format((x.Spelling_or_Grammar / x.z_total_people).sum(), '.2%'),\n",
    "          Appeal_to_Greed_Ratio = format((x.Appeal_to_Greed / x.z_total_people).sum(), '.2%'),\n",
    "          Appeal_to_Urgency_Ratio = format((x.Appeal_to_Urgency / x.z_total_people).sum(), '.2%'),\n",
    "          Appeal_to_Authority_Ratio = format((x.Appeal_to_Authority / x.z_total_people).sum(), '.2%'),\n",
    "          Other_Phishy_Findings_Ratio = format((x.Other_Phishy_Findings / x.z_total_people).sum(), '.2%'),                             \n",
    "          Total_People = x.z_total_people.sum()\n",
    "    ))).reset_index(\"subject_id\")\n",
    "\n",
    "z_cue_percentages_by_people = pd.DataFrame(z_cue_percentages_by_people)\n",
    "\n",
    "z_cue_percentages_by_people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37788b",
   "metadata": {},
   "source": [
    "CSV for Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_cue_percentages_by_people.to_csv(\"Cue_perc_by_people.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8b942",
   "metadata": {},
   "source": [
    "Beginning of Version 1.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Getting the total number of none duplicate cues for every image\n",
    "z_total_cues = sortedcues.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict(\n",
    "        total_cues_for_image = (x.Invalid_Domain_or_Sender + x.Potent_Mal_Links + x.Spelling_or_Grammar + x.Appeal_to_Greed + x.Appeal_to_Urgency + x.Appeal_to_Authority + x.Other_Phishy_Findings).sum()\n",
    "    ))).reset_index(\"subject_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 \n",
    "# Mergeing the none duplicate cues given and the total number of cues given for a image\n",
    "sortedcues_total_cue = pd.merge(sortedcues_gold_std, z_total_cues, on=\"subject_id\")\n",
    "sortedcues_total_cue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcb63e",
   "metadata": {},
   "source": [
    "Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 \n",
    "# Creating the ratio table \n",
    "# Cue type / total cues for image\n",
    "z_cue_percentages_by_cue = sortedcues_total_cue.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict( filename = x.filename.sum(),\n",
    "          malicious = (x.malicious.sum()  == 1),\n",
    "          gold_std = (x.gold_std.sum() == 1),\n",
    "          Invalid_Domain_Sender_Ratio = format((x.Invalid_Domain_or_Sender / x.total_cues_for_image).sum(),'.2%'),\n",
    "          Potential_Malicious_Links_Ratio = format((x.Potent_Mal_Links / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Poor_Spelling_or_Grammar_Ratio = format((x.Spelling_or_Grammar / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Appeal_to_Greed_Ratio = format((x.Appeal_to_Greed / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Appeal_to_Urgency_Ratio = format((x.Appeal_to_Urgency / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Appeal_to_Authority_Ratio = format((x.Appeal_to_Authority / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Other_Phishy_Findings_Ratio = format((x.Other_Phishy_Findings / x.total_cues_for_image).sum(), '.2%'),                             \n",
    "          Total_Cues_for_Image = x.total_cues_for_image.sum()\n",
    "    ))).reset_index(\"subject_id\")\n",
    "\n",
    "z_cue_percentages_by_cue = pd.DataFrame(z_cue_percentages_by_cue)\n",
    "\n",
    "z_cue_percentages_by_cue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c747b3",
   "metadata": {},
   "source": [
    "CSV for Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#z_cue_percentages_by_cue.to_csv(\"Cue_perc_by_cue.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab4498",
   "metadata": {},
   "source": [
    "Beginning of Verision 1.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cde9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying zooniverseclassification dataframe before sorting it\n",
    "zooniverseclassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataframe \n",
    "userboundingboxpos = pd.DataFrame(columns=[\"user_labeled_image\", \"subject_id\",\"x_pos\", \"y_pos\", \"width\", \"height\", \"cue\"])\n",
    "\n",
    "# Sorting the data store in zooniverse classifications into the userboundingboxpos dataframe\n",
    "for user_labeled_image in range(0, len(zooniverseclassification[\"user_answers\"])):\n",
    "    current_string = zooniverseclassification[\"user_answers\"][user_labeled_image]\n",
    "    json_object = json.loads(current_string)\n",
    "    \n",
    "    # Holds the cues that a person put on a single image during their session\n",
    "    # temp_list holds a tuple\n",
    "    # the tuple are structured (cue type, x_pos, y_pos, width, height)\n",
    "    temp_list = []\n",
    "    cue_is_unique = True\n",
    "\n",
    "    for item in json_object:\n",
    "        \n",
    "\n",
    "        for cue in range(0, len(temp_list)):\n",
    "                \n",
    "            # This compares the current cue's type being added with cues' that have already been added to the dataframe from the user\n",
    "            # Checking to see if the cue's are of the same type\n",
    "            if item[\"tool_label\"] == temp_list[cue][0]:\n",
    "                # if the cues are of the same type, check that they are intersecting by Separating axis test\n",
    "                if (\n",
    "                    (item['x'] > (temp_list[cue][1] + temp_list[cue][3])) and \n",
    "                    (temp_list[cue][1] > (item['x'] + item['width'])) and\n",
    "                    (item['y'] > (temp_list[cue][2] + temp_list[cue][4])) and\n",
    "                    (temp_list[cue][2] > (item['y'] + item['height']))\n",
    "                ):\n",
    "                    cue_is_unique = False\n",
    "        \n",
    "        # adds the cue to the data frame if it is unique        \n",
    "        if cue_is_unique:\n",
    "            temp_list.append((item[\"tool_label\"], item[\"x\"], item[\"y\"], item[\"width\"],item[\"height\"]))\n",
    "            \n",
    "            data_input = {\"user_labeled_image\": user_labeled_image, \n",
    "                            \"subject_id\": zooniverseclassification[\"subject_id\"][user_labeled_image], \n",
    "                            \"x_pos\": item[\"x\"],\n",
    "                            \"y_pos\": item[\"y\"],\n",
    "                            \"width\": item[\"width\"],\n",
    "                            \"height\": item[\"height\"],\n",
    "                            \"cue\": item[\"tool_label\"]}   \n",
    "        \n",
    "            userboundingboxpos.loc[len(userboundingboxpos.index)] = data_input\n",
    "\n",
    "userboundingboxpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping all the cues together\n",
    "V_subject_cues = userboundingboxpos.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict(Invalid_Domain_or_Sender = (x.cue == \"Invalid Domain or Sender\").sum(),\n",
    "         Potent_Mal_Links = (x.cue == \"Potentially Malicious Link\").sum(),\n",
    "         Spelling_or_Grammar = (x.cue == \"Poor Spelling or Grammar\").sum(),\n",
    "         Appeal_to_Greed = (x.cue == \"Appeal to Action-Greed\").sum(),\n",
    "         Appeal_to_Urgency = (x.cue == \"Appeal to Action-Urgency\").sum(),\n",
    "         Appeal_to_Authority = (x.cue == \"Appeal to Action-Authority\").sum(),\n",
    "         Other_Phishy_Findings = (x.cue == \"Other Phishy Findings\").sum()\n",
    "        ))).reset_index(\"subject_id\")\n",
    "\n",
    "V_subject_cues = pd.DataFrame(V_subject_cues)\n",
    "\n",
    "V_subject_cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e615a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with gold standard\n",
    "change_type ={\"subject_id\": \"int64\"}\n",
    "V_subject_cues = V_subject_cues.astype(change_type)\n",
    "\n",
    "V_gold_std = pd.merge(gold_std, V_subject_cues, on='subject_id')\n",
    "V_gold_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d918449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting total number of cues\n",
    "V_total_cues = V_gold_std.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict(\n",
    "        total_cues_for_image = (x.Invalid_Domain_or_Sender + x.Potent_Mal_Links + x.Spelling_or_Grammar + x.Appeal_to_Greed + x.Appeal_to_Urgency + x.Appeal_to_Authority + x.Other_Phishy_Findings).sum()\n",
    "    ))).reset_index(\"subject_id\")\n",
    "\n",
    "V_total_cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_gold_std_total_cues = pd.merge(V_gold_std, V_total_cues, on='subject_id')\n",
    "V_gold_std_total_cues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fdca37",
   "metadata": {},
   "source": [
    "Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Table\n",
    "V_cue_ratio = V_gold_std_total_cues.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict( filename = x.filename.sum(),\n",
    "          malicious = (x.malicious.sum()  == 1),\n",
    "          gold_std = (x.gold_std.sum() == 1),\n",
    "          Invalid_Domain_Sender_Ratio = format((x.Invalid_Domain_or_Sender / x.total_cues_for_image).sum(),'.2%'),\n",
    "          Potential_Malicious_Links_Ratio = format((x.Potent_Mal_Links / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Poor_Spelling_or_Grammar_Ratio = format((x.Spelling_or_Grammar / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Appeal_to_Greed_Ratio = format((x.Appeal_to_Greed / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Appeal_to_Urgency_Ratio = format((x.Appeal_to_Urgency / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Appeal_to_Authority_Ratio = format((x.Appeal_to_Authority / x.total_cues_for_image).sum(), '.2%'),\n",
    "          Other_Phishy_Findings_Ratio = format((x.Other_Phishy_Findings / x.total_cues_for_image).sum(), '.2%'),                             \n",
    "          Total_Cues_for_Image = x.total_cues_for_image.sum()\n",
    "    ))).reset_index(\"subject_id\")\n",
    "\n",
    "V_cue_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcd0aa",
   "metadata": {},
   "source": [
    "Getting Dataframe for \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedcues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f63328",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_total_people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images_labeled_with_people= pd.merge(sortedcues, z_total_people_df, on='subject_id')\n",
    "total_images_labeled_with_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44313dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cue_over_people_ratio = total_images_labeled_with_people.groupby(\"subject_id\").apply(lambda x: pd.Series(\n",
    "    dict(\n",
    "          Invalid_Domain_Sender_Ratio = format((x.Invalid_Domain_or_Sender / x.z_total_people).sum(),'.2%'),\n",
    "          Potential_Malicious_Links_Ratio = format((x.Potent_Mal_Links / x.z_total_people).sum(), '.2%'),\n",
    "          Poor_Spelling_or_Grammar_Ratio = format((x.Spelling_or_Grammar / x.z_total_people).sum(), '.2%'),\n",
    "          Appeal_to_Greed_Ratio = format((x.Appeal_to_Greed / x.z_total_people).sum(), '.2%'),\n",
    "          Appeal_to_Urgency_Ratio = format((x.Appeal_to_Urgency / x.z_total_people).sum(), '.2%'),\n",
    "          Appeal_to_Authority_Ratio = format((x.Appeal_to_Authority / x.z_total_people).sum(), '.2%'),\n",
    "          Other_Phishy_Findings_Ratio = format((x.Other_Phishy_Findings / x.z_total_people).sum(), '.2%'),                             \n",
    "          Total_People = x.z_total_people.sum()\n",
    "\n",
    "    ))).reset_index(\"subject_id\")\n",
    "\n",
    "all_cue_over_people_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns with percentages\n",
    "percentage_columns = all_cue_over_people_ratio.iloc[:, 1:8]\n",
    "\n",
    "# Get the name of the column with the maximum percentage for each row\n",
    "column_with_max_percentage = percentage_columns.idxmax(axis=1)\n",
    "\n",
    "# Add the result as a new column to the dataframe\n",
    "all_cue_over_people_ratio['Column_With_Max_Percentage'] = column_with_max_percentage\n",
    "\n",
    "all_cue_over_people_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_counts = all_cue_over_people_ratio['Column_With_Max_Percentage'].value_counts()\n",
    "\n",
    "# Create a new dataframe from the counts\n",
    "count_df = pd.DataFrame({'Column_Name': column_counts.index, 'Count': column_counts.values})\n",
    "\n",
    "# Calculate the total count for all rows\n",
    "total_count = count_df['Count'].sum()\n",
    "\n",
    "# Add a new column for the ratio in count_df\n",
    "count_df['Ratio'] = count_df['Count'] / total_count\n",
    "\n",
    "# Display the new dataframe\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#count_df.to_csv(\"Cue_prec_by_people_freq.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07499d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedcues_total_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_trust_no_trust = classificationsession.groupby('subject_id').apply(lambda x: pd.Series(\n",
    "               dict(z_trust = (x.zoo_trust_b ==  True).sum(),\n",
    "                    z_no_trust =(x.zoo_trust_b == False).sum()\n",
    "                    ))).reset_index('subject_id')\n",
    "z_trust_no_trust = pd.DataFrame(z_trust_no_trust)\n",
    "\n",
    "z_trust_no_trust[z_trust_no_trust['subject_id'] == 43857918\t\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93540837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedcues_total_people = pd.merge(sortedcues_total_people, z_trust_no_trust, on= 'subject_id')\n",
    "sortedcues_total_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedcues_total_people.to_csv(\"Distinct_people_cue_gld_std.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
